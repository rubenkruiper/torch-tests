{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic steps of setting up a pytorch model\n",
    "1. __Data:__ preprocessing is outside the scope of this tutorial, the input is expect to be some torch tensor type\n",
    "2. __Network:__ input parameters, target parameters\n",
    "3. __Loss function__\n",
    "4. __Optimiser__\n",
    "5. __Training function__\n",
    "6. __Prediction function:__ use the trained network to make predictions on unseen input\n",
    "7. __Train the network__ using the given loss function and optimiser\n",
    "8. __Plot__ the losses, accuracy, etc..\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1 - Data\n",
    "Working with torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor as seed 0: \n",
      " 0.5488  0.5928\n",
      " 0.7152  0.8443\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Different random tensor without seed 0: \n",
      " 0.6028  0.8579\n",
      " 0.5449  0.8473\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n",
      "Same tensor as the first one, because we use seed 0: \n",
      " 0.5488  0.5928\n",
      " 0.7152  0.8443\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# every seed produces the same random array\n",
    "torch.manual_seed(0)\n",
    "tensor_1 = torch.rand(2,2)\n",
    "\n",
    "tensor_2 = torch.rand(2,2)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "tensor_3 = torch.rand(2,2)\n",
    "\n",
    "print('Random tensor as seed 0:', tensor_1)\n",
    "print('Different random tensor without seed 0:',tensor_2)\n",
    "print('Same tensor as the first one, because we use seed 0:',tensor_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random tensor as seed 0: \n",
      " 0.0202  0.3682\n",
      " 0.8326  0.9572\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Random tensor as seed 0: \n",
      " 0.7782  0.1404\n",
      " 0.8700  0.8701\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    tensor_4 = torch.rand(2,2).cuda()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    tensor_5 = torch.rand(2,2).cuda()\n",
    "    \n",
    "print('Random tensor as seed 0:', tensor_4)\n",
    "print('Random tensor as seed 0:', tensor_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.cuda.FloatTensor'>\n",
      "<class 'torch.FloatTensor'>\n"
     ]
    }
   ],
   "source": [
    "# Tensors on CPU vs GPU\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    tensor_1 = tensor_1.cuda()  # convert to gpu tensor with .cuda()\n",
    "\n",
    "print(type(tensor_1))\n",
    "\n",
    "tensor_1 = tensor_1.cpu()  # back to CPU with .cpu()\n",
    "print(type(tensor_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (re)size\n",
    "###### inplace is faster due to memory efficiency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.94 µs ± 30.2 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4 - tensor_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.61 µs ± 15.5 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4.sub(tensor_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.29 µs ± 5.21 ns per loop (mean ± std. dev. of 7 runs, 100000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit tensor_4.sub_(tensor_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### subtract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 2])\n",
      "<class 'torch.cuda.FloatTensor'>\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "print(tensor_1.size()) \n",
    "print(type(tensor_4)) # size works the same on cuda tensors\n",
    "print(tensor_4.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0.5690  0.9611\n",
      " 1.5478  1.8014\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      " 1.0050  0.8094\n",
      " 1.2836  0.9795\n",
      "[torch.FloatTensor of size 2x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tensor_4.add(tensor_1.cuda())) # to add two tensor, they have to be the same type\n",
    "print(tensor_1.add(torch.rand(2,2).float())) # both GPU/CPU and float/long/double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-5.9757 -1.2071\n",
      "-5.9727 -5.9777\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "-5.1975 -1.0667\n",
      "-5.1026 -5.1076\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "-5.9757 -1.2071\n",
      "-5.9727 -5.9777\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "\n",
      "-5.9757 -1.2071\n",
      "-5.9727 -5.9777\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(tensor_4.sub(tensor_5))\n",
    "print(tensor_4)                 # sub subtracts tensor_4 with tensor_5 == tensor_4 - tensor_5\n",
    "print(tensor_4.sub_(tensor_5))\n",
    "print(tensor_4)                 # sub_ replaces tensor_4 with tensor_5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### multiply elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor_4: \n",
      "1.00000e+06 *\n",
      " -1.1176 -0.0066\n",
      " -1.6075 -1.6078\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "tensor_5: \n",
      " 0.7782  0.1404\n",
      " 0.8700  0.8701\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply elementwise: \n",
      "1.00000e+06 *\n",
      " -0.8697 -0.0009\n",
      " -1.3986 -1.3989\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply elementwise: \n",
      "1.00000e+06 *\n",
      " -0.8697 -0.0009\n",
      " -1.3986 -1.3989\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n",
      "Multiply inplace: \n",
      "1.00000e+06 *\n",
      " -0.8697 -0.0009\n",
      " -1.3986 -1.3989\n",
      "[torch.cuda.FloatTensor of size 2x2 (GPU 0)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('tensor_4:',tensor_4)\n",
    "print('tensor_5:',tensor_5)\n",
    "# mul multiplies tensor_4 with tensor_5 == tensor_4 - tensor_5\n",
    "print('Multiply elementwise:',tensor_4.mul(tensor_5))    \n",
    "print('Multiply elementwise:',tensor_4 * tensor_5)\n",
    "    # mul_ without using additional memory \n",
    "print('Multiply inplace:',tensor_4.mul_(tensor_5))    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### division elementwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2 - Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ab5b16e228bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocabulary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "#parameters\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, max_len, embedding_size, num_channels, output_size, dropout):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # initialise a Network object containing e.g. the following layers  \n",
    "            # an embedding layer \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "            # a Linear hidden layer \n",
    "        self.hidden = nn.Linear(input_size, output_size)\n",
    "            # a convolutional layer taking input with 1 channel\n",
    "        self.conv = nn.Conv2d(in_channels=1, out_channels=num_channels, kernel_size=(3,embedding_size))\n",
    "            # an LSTM taking word embeddings as inputs, outputting hidden states\n",
    "        self.lstm = nn.LSTM(embedding_dimension, hidden_dimension)\n",
    "            # a quasi-recurrent NN input: 100, output: 200, dropout\n",
    "        self.qrnn = QRNN(100, 200, dropout=.5) \n",
    "        \n",
    "    \n",
    "    def init_hidden(self):\n",
    "        # Initialise hidden states for an lstm\n",
    "        # The axes semantics are (num_layers, minibatch_size, hidden_dim)\n",
    "        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n",
    "                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # determine how the input is passed through the layers\n",
    "        # in example for input x, where x is a sentence        \n",
    "            # convert the vector of words to a vector of word embeddings\n",
    "        word_embeddings = self.embedding(x)\n",
    "            # run the word_embedding vector through an lstm\n",
    "        lstm_out, self.hidden = self.lstm(word_embeddings.view(len(x), 1, -1), self.hidden)\n",
    "            # activation function for the convolutional layer: relu\n",
    "            # the squeeze removes 1 argument / decrease the arity, \n",
    "            #      e.g. [batch_size,num_channels, Height, Width]\n",
    "            #      becomes [batch_size,num_channels, Height]\n",
    "        x_conv = F.relu(self.conv(x)).squeeze(3)\n",
    "            # \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
